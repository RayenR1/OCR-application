{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "# Initialize Qari-OCR model\n",
    "model_name = \"NAMAA-Space/Qari-OCR-0.2.2.1-VL-2B-Instruct\"\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "max_tokens = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Text:\n",
      "Bulletin de soins N° 9550517 مَعالجة بطاقة\n",
      "Réserve à l'adhérent خاص بالمنخرط\n",
      "Nom et prénom de l'adhérent مَا بِسَدِ اسم و لقب المنخرط\n",
      "Numéro CIN ou passeport 6137311 عدد بطاقة التعريف الوطنية أو جواز السفر\n",
      "6 4 9 0 8 لمج سوسة عدد 16 8 1998 السفر جوّار بطاقة التعرف الوطنية أو عنوان المنخرط\n",
      "Adresse de l'adhérent : ∞\n",
      "Matricule CNAM N° 16×76×4908 عدد معرف الصندوق الوطني للتأمين على المرض\n",
      "Matricule de l'adhérent 574\n",
      "رقم المنخرط\n",
      "Code prestataire 00\n",
      "Adhérent 00 Conjoint 99 [*] Signature de l'Adhérent إمضاء المنخرط\n",
      "(*) : Enfant 1er : 01 - 2* : 02 : 03 . 3 e : 03 . 3 e etc\n",
      "réservé aux médecins et praticiens خاص بالأطباء والممارسين\n",
      "Nom et prénom du malade ab euv SA و لقب المريض اسم\n",
      "Date de naissance 0 3 - 0 3 - 0 3 تاريخ الولادة\n",
      "تاريخ Pate Date نوعية العلاج Désignation Honoraires الأتعاب الإضاء و الختم Visa & cachet Visa & < Matricule fiscal المعرف الجسائي\n",
      "ريح Date نوعية العلاج Désignation نوعيه Honoraires الآتِاب Visa & المضاء و أ & cachet Matricule fiscal المعرف الجبائي\n",
      "En cas de chirurgie ou de traitements spéciaux, الخصائِصِ الخاصِ حالات الجراحة أو العلاج الطبيعي\n",
      "il faut joindre au bulletin de soins, la nature de l'acte أنجز، بـن نوع العمل الا العلاج، بِمَنْزِلِهِ الد يجب مصاحبة بطاقة الـ ي\n",
      "pratiqué sous pli confidentiel. في ظرف سرّي.\n",
      "خاص Réserve à l'établissement hospitalier de l'Établissement hospitalier de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission de la Mission\n",
      "تاريخ الدخول Date d'entrée\n",
      "تاريخ ا الخروج Date de sortie\n",
      "مقدار المصاريف Montant des frais\n",
      "المعرف الجبائي Matricule Fiscal\n",
      "Visa et cachet de l'établissement hospitalier\n",
      "إمضاء وختم المستشفى أو المصحة\n",
      "خاص بتأمينات ستار réservé à la star\n",
      "المصاريف رمز العنوان\n",
      "إيمضاء وخاتم المراقب Visa & cachet du vérificateur\n",
      "المصاريف Frais engagés زمر العثول Code rubrique\n",
      "OBSERVATIONS\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = r\"C:\\Users\\jlassi\\Desktop\\EyeQ_app\\data\\BS\\BsStar\\0741--9550517--20230705_page_0.jpg\" # Replace with your image path\n",
    "JSON_PATH = r\"C:\\Users\\jlassi\\Desktop\\testYolo\\notebooks\\mmm\\combined_layout.json\"  # Path to JSON file\n",
    "CONFIDENCE_THRESHOLD = 0.80  # Confidence threshold for EasyOCR\n",
    "OUTPUT_TEXT_PATH = \"document_text.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "if image is None:\n",
    "    raise FileNotFoundError(f\"Image not found at {IMAGE_PATH}\")\n",
    "image_height, image_width = image.shape[:2]\n",
    "\n",
    "# Initialize EasyOCR for Arabic and English\n",
    "reader = easyocr.Reader(['ar', 'en'])\n",
    "\n",
    "# Load JSON layout\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    layout_data = json.load(f)\n",
    "\n",
    "lines = layout_data.get(\"lines\", [])\n",
    "tables = layout_data.get(\"tables\", {})\n",
    "\n",
    "# Function to crop image with bounds checking\n",
    "def safe_crop(image, bbox):\n",
    "    x_min, y_min, x_max, y_max = [int(x) for x in bbox]\n",
    "    x_min = max(0, x_min - 10)  # Add padding\n",
    "    y_min = max(0, y_min - 10)\n",
    "    x_max = min(image.shape[1], x_max + 10)\n",
    "    y_max = min(image.shape[0], y_max + 10)\n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        return None\n",
    "    return image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "# Function to apply OCR to a cropped region\n",
    "def extract_text_from_crop(crop, reader, model, processor):\n",
    "    if crop is None:\n",
    "        return []\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    easyocr_results = reader.readtext(thresh, detail=1, paragraph=False)\n",
    "    texts = []\n",
    "    for (bbox, text, prob) in easyocr_results:\n",
    "        (top_left, top_right, bottom_right, bottom_left) = bbox\n",
    "        top_left = (int(top_left[0]), int(top_left[1]))\n",
    "        bottom_right = (int(bottom_right[0]), int(bottom_right[1]))\n",
    "        sub_crop = crop[max(0, top_left[1]-5):bottom_right[1]+5, max(0, top_left[0]-5):bottom_right[0]+5]\n",
    "        if sub_crop.size == 0:\n",
    "            continue\n",
    "        crop_pil = Image.fromarray(cv2.cvtColor(sub_crop, cv2.COLOR_BGR2RGB))\n",
    "        temp_img_path = \"temp_crop.png\"\n",
    "        crop_pil.save(temp_img_path)\n",
    "        prompt = \"Extract the plain text from the provided image as if you were reading it naturally. Do not hallucinate.\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": f\"file://{temp_img_path}\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text_input],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        qari_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0].strip()\n",
    "        os.remove(temp_img_path)\n",
    "        final_text = qari_text or text\n",
    "        if final_text:\n",
    "            texts.append({\n",
    "                \"text\": final_text,\n",
    "                \"box\": [top_left[0], top_left[1], bottom_right[0], bottom_right[1]]\n",
    "            })\n",
    "    return texts\n",
    "\n",
    "# Identify table headers (line immediately above table)\n",
    "table_headers = {}\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    table_y_min = table_bbox[1]\n",
    "    header_line = None\n",
    "    min_y_diff = float(\"inf\")\n",
    "    for i, line in enumerate(lines):\n",
    "        line_y_max = line[\"bounding_box\"][3]\n",
    "        if line_y_max <= table_y_min and table_y_min - line_y_max < min_y_diff:\n",
    "            min_y_diff = table_y_min - line_y_max\n",
    "            header_line = i\n",
    "    if header_line is not None and min_y_diff < 50:\n",
    "        table_headers[table_key] = header_line\n",
    "\n",
    "# Group lines by left and right side\n",
    "midpoint = image_width / 2\n",
    "left_lines = []\n",
    "right_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    line_x_min = line[\"bounding_box\"][0]\n",
    "    if line_x_min < midpoint:\n",
    "        left_lines.append(i)\n",
    "    else:\n",
    "        right_lines.append(i)\n",
    "\n",
    "# Process lines\n",
    "line_texts = [[] for _ in lines]\n",
    "for i, line in enumerate(lines):\n",
    "    if i in table_headers.values():\n",
    "        continue  # Skip header lines, processed with tables\n",
    "    bbox = line[\"bounding_box\"]\n",
    "    crop = safe_crop(image, bbox)\n",
    "    texts = extract_text_from_crop(crop, reader, model, processor)\n",
    "    # Adjust box coordinates to global image coordinates\n",
    "    for text in texts:\n",
    "        text[\"box\"][0] += bbox[0]  # x_min\n",
    "        text[\"box\"][2] += bbox[0]  # x_max\n",
    "        text[\"box\"][1] += bbox[1]  # y_min\n",
    "        text[\"box\"][3] += bbox[1]  # y_max\n",
    "    line_texts[i] = texts\n",
    "\n",
    "# Process tables\n",
    "table_texts = {key: [] for key in tables}\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    crop = safe_crop(image, table_bbox)\n",
    "    texts = extract_text_from_crop(crop, reader, model, processor)\n",
    "    # Adjust box coordinates to global image coordinates\n",
    "    for text in texts:\n",
    "        text[\"box\"][0] += table_bbox[0]\n",
    "        text[\"box\"][2] += table_bbox[0]\n",
    "        text[\"box\"][1] += table_bbox[1]\n",
    "        text[\"box\"][3] += table_bbox[1]\n",
    "    table_texts[table_key] = texts\n",
    "    # Process header\n",
    "    header_idx = table_headers.get(table_key)\n",
    "    if header_idx is not None:\n",
    "        header_bbox = lines[header_idx][\"bounding_box\"]\n",
    "        crop = safe_crop(image, header_bbox)\n",
    "        header_texts = extract_text_from_crop(crop, reader, model, processor)\n",
    "        for text in header_texts:\n",
    "            text[\"box\"][0] += header_bbox[0]\n",
    "            text[\"box\"][2] += header_bbox[0]\n",
    "            text[\"box\"][1] += header_bbox[1]\n",
    "            text[\"box\"][3] += header_bbox[1]\n",
    "        line_texts[header_idx] = header_texts\n",
    "\n",
    "# Format output\n",
    "formatted_text = []\n",
    "\n",
    "# Process left-side lines\n",
    "for line_idx in sorted(left_lines, key=lambda i: lines[i][\"bounding_box\"][1]):\n",
    "    if line_idx in table_headers.values():\n",
    "        continue\n",
    "    texts = line_texts[line_idx]\n",
    "    if not texts:\n",
    "        continue\n",
    "    texts.sort(key=lambda t: t[\"box\"][0])  # Left-to-right\n",
    "    line_text = \" \".join(t[\"text\"] for t in texts if t[\"text\"].strip())\n",
    "    if line_text:\n",
    "        formatted_text.append(line_text)\n",
    "\n",
    "# Process right-side lines\n",
    "for line_idx in sorted(right_lines, key=lambda i: lines[i][\"bounding_box\"][1]):\n",
    "    if line_idx in table_headers.values():\n",
    "        continue\n",
    "    texts = line_texts[line_idx]\n",
    "    if not texts:\n",
    "        continue\n",
    "    texts.sort(key=lambda t: -t[\"box\"][0])  # Right-to-left for Arabic\n",
    "    line_text = \" \".join(t[\"text\"] for t in texts if t[\"text\"].strip())\n",
    "    if line_text:\n",
    "        formatted_text.append(line_text)\n",
    "\n",
    "# Process tables\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    texts = table_texts[table_key]\n",
    "    if not texts:\n",
    "        continue\n",
    "    col_names = []\n",
    "    col_positions = []\n",
    "    header_idx = table_headers.get(table_key)\n",
    "    if header_idx is not None:\n",
    "        header_texts = line_texts[header_idx]\n",
    "        header_texts.sort(key=lambda t: t[\"box\"][0])\n",
    "        col_names = [t[\"text\"] for t in header_texts if t[\"text\"].strip()]\n",
    "        col_positions = [(t[\"box\"][0] + t[\"box\"][2]) / 2 for t in header_texts]\n",
    "    texts.sort(key=lambda t: t[\"box\"][1])\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    current_y = None\n",
    "    y_tolerance = 20\n",
    "    for text in texts:\n",
    "        y = text[\"box\"][1]\n",
    "        if current_y is None or abs(y - current_y) < y_tolerance:\n",
    "            current_row.append(text)\n",
    "            current_y = y if current_y is None else current_y\n",
    "        else:\n",
    "            if current_row:\n",
    "                rows.append(current_row)\n",
    "            current_row = [text]\n",
    "            current_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    aligned_rows = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda t: t[\"box\"][0])\n",
    "        row_positions = [(t[\"box\"][0] + t[\"box\"][2]) / 2 for t in row]\n",
    "        aligned_row = [\"\"] * len(col_names)\n",
    "        for text, pos in zip([t[\"text\"] for t in row], row_positions):\n",
    "            if col_positions:\n",
    "                closest_col_idx = min(range(len(col_positions)), key=lambda i: abs(col_positions[i] - pos))\n",
    "                aligned_row[closest_col_idx] = text\n",
    "        aligned_rows.append(aligned_row)\n",
    "    max_widths = [max(len(col_names[i]) if i < len(col_names) else 0,\n",
    "                      max((len(row[i]) if i < len(row) else 0) for row in aligned_rows))\n",
    "                  for i in range(max(len(col_names), max(len(row) for row in aligned_rows)))]\n",
    "    if col_names:\n",
    "        header_row = \" | \".join(col_names[i].ljust(max_widths[i]) if i < len(col_names) else \"\".ljust(max_widths[i])\n",
    "                               for i in range(len(max_widths)))\n",
    "        formatted_text.append(header_row)\n",
    "        formatted_text.append(\"-\" * len(header_row))\n",
    "    for row in aligned_rows:\n",
    "        row_text = \" | \".join(row[i].ljust(max_widths[i]) if i < len(row) else \"\".ljust(max_widths[i])\n",
    "                             for i in range(len(max_widths)))\n",
    "        formatted_text.append(row_text)\n",
    "\n",
    "# Save and print formatted text\n",
    "with open(OUTPUT_TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(formatted_text))\n",
    "\n",
    "print(\"Formatted Text:\")\n",
    "print(\"\\n\".join(formatted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formalized Text:\n",
      "**Patient Information**\n",
      "\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Date of Admission | [Insert date] |\n",
      "| Date of Discharge | [Insert date] |\n",
      "| Total Expenses | [Insert amount] |\n",
      "| Fiscal Identification Number | [Insert number] |\n",
      "\n",
      "**Visa and Cachet from the Hospital or Clinic**\n",
      "\n",
      "* Signature and Seal of the Hospital or Clinic: [Insert signature and seal]\n",
      "\n",
      "**Reserved for Star Insurance**\n",
      "\n",
      "* [No content to preserve, as this section is reserved for insurance purposes]\n",
      "\n",
      "**Expenses Code Rubric**\n",
      "\n",
      "| Code | Description |\n",
      "| --- | --- |\n",
      "| [Insert code] | [Insert description] |\n",
      "\n",
      "**Observations**\n",
      "\n",
      "[Insert observations or comments from the healthcare provider]\n",
      "\n",
      "Note: I removed the repeated lines of text and formatted the output into a clean, structured format with sections and tables. I also preserved all valid content except for the specified hallucinations (Reserved for Star Insurance).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "EXTRACTED_TEXT_PATH = \"document_text.txt\"\n",
    "FORMALIZED_TEXT_PATH = \"formalized_text.txt\"\n",
    "\n",
    "# Read the extracted text\n",
    "with open(EXTRACTED_TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted_text = f.read()\n",
    "\n",
    "# Define the prompt for llama3 via Ollama\n",
    "prompt = f\"\"\"\n",
    "### Context:\n",
    "You are an advanced language model tasked with cleaning up and formalizing text extracted from a medical care form (Bulletin de soins). The form contains information in both Arabic and French, related to patient details, hospital information, and a table of medical acts. The extracted text contains specific errors such as:\n",
    "- Long sequences of numbers (e.g., '86, 87, 88, ...') that are hallucinations.\n",
    "- Japanese text (e.g., '## 2020.04.18 ## ## 11 月 1 1 日') that doesn't belong.\n",
    "- Misaligned or poorly formatted lines and tables.\n",
    "\n",
    "### Task:\n",
    "Your task is to:\n",
    "1. Clean up the extracted text by removing only the specific hallucinations (long number sequences and Japanese text) while preserving all other valid content.\n",
    "2. Correct language inconsistencies (ensure only Arabic and French text remains, with proper context).\n",
    "3. Organize the text into a formal structure, respecting the form’s sections (e.g., patient info, hospital info, tables).\n",
    "4. Ensure proper formatting for tables, with aligned columns and headers.\n",
    "5. Preserve the original meaning and content as much as possible, avoiding deletion of valid text.\n",
    "\n",
    "### Types of Corrections:\n",
    "- **Remove Hallucinations**: Remove long sequences of numbers (e.g., '86, 87, 88, ...') and Japanese text (e.g., '## 2020.04.18 ## ## 11 月 1 1 日'), but keep all other text.\n",
    "- **Language Consistency**: Ensure Arabic text is coherent and French text is coherent. Remove only non-Arabic/French text (e.g., Japanese).\n",
    "- **Formal Structure**: Organize the text into clear sections (e.g., 'Patient Information', 'Hospital Information', 'Medical Acts Table').\n",
    "- **Table Formatting**: Ensure the table has proper headers and aligned rows.\n",
    "- **Text Correction**: Fix obvious OCR errors (e.g., 'Adhérent 00 Conjoint' should be separated into fields).\n",
    "\n",
    "### Extracted Text:\n",
    "{extracted_text}\n",
    "\n",
    "### Output:\n",
    "Provide the formalized text in a clean, structured format with sections and tables properly formatted, preserving all valid content except the specified hallucinations.\n",
    "\"\"\"\n",
    "\n",
    "# Use Ollama to process the extracted text with llama3\n",
    "response = ollama.generate(\n",
    "    model=\"llama3\",\n",
    "    prompt=prompt,\n",
    "    options={\n",
    "        \"temperature\": 0.0,  # Disable randomness for deterministic output\n",
    "        \"num_predict\": 1500,  # Max tokens to generate\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the formalized text from the response\n",
    "formalized_text = response[\"response\"].split(\"### Output:\")[-1].strip() if \"### Output:\" in response[\"response\"] else response[\"response\"].strip()\n",
    "\n",
    "# Save the formalized text\n",
    "with open(FORMALIZED_TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(formalized_text)\n",
    "\n",
    "print(\"Formalized Text:\")\n",
    "print(formalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Text:\n",
      "Bulletin de soins N° 955051 7 بطاقة معالجة\n",
      "réservé à l'adhérent بالمنخرط خاص بـ\n",
      "Nom et prénom de l'adhérent ‎AU‏ ا Handwritten text not recognized لقب اب 6 9 اسم و\n",
      "Numéro CIN ou passeport ©7521 عدد بطاقة التعريف الوطنية أو جواز السفر\n",
      "عدد 16 سايل 2 4 9 0 8 — ——— اا\n",
      "‎Ont‏\n",
      "‎١ 0‏ عدد بطاقة التعريف الوطنية أو جوّار السفر أَجِج لسَّد سَّد لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّدِّ لِلصَّ عنوان المنخرط\n",
      "Adresse de l'entreprise : le l'adhérent 8000\n",
      "Matricule CNAM N° 16 7 6 4 9 0 8 عدد على المرض ع لأول مرة، أتمنى لكم التأمين العظيم معرف الصندوق الوطني ل\n",
      "Matricule de l'adhérent Le\n",
      "رقم المنخرط\n",
      "Code prestataire 00 99\n",
      "Adhére nt 00 Conjoint 99 / Signature de l'Adhérent إمضاء المنخرط\n",
      "(*) : Enfant 1er  : 01 - 2* : 02 3:03 etc\n",
      "Réserve aux médecins et praticiens بالأطباء والممارسين خاص بـ\n",
      "Nom et prénom du malade حصنا وطن 5 فعمفقت و لقب المريض أسم (اسم)\n",
      "Date de naissance 0 3 - 0 3 تاريخ الولادة\n",
      "تاريخ Pate Date العلاج Désignation نوعية Honoraires الأتعاب المضاء و الختم Visa & rachat Visä.& ‏عق‎ ST\n",
      "”\n",
      "De) الإمساك بالرقبة # Matricule fiscal المعرف الجبائي فم غيب\n",
      "‎xe.‏\n",
      "ويا Date نوعيه العلاج Résignation Désignation Honoraires الانسعهاب Visä:&'cachét المَضاء و 1 & rachet ke Matricule fiscal المَعرف الجبَائيُّ لِلْجِنْدِيِّ\n",
      "En cas de chirurgie ou de traitements spéciaux, مَنِّيَ الخاص في حالات الجراحة أو العلاج الطبي\n",
      "il faut joindre au bulletin de soins, la nature de l'acte Handwritten text not recognized بِنوع العمل 1-20 لعلاج، يُنصح بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَنْحِهِمْ، بِمَ د 2 ! بطة ابْطَاء Handwritten text not recognized يجب مصاحبة LA\n",
      "pratiqué sous pli confidentiel. في ظرف سرّي.\n",
      "خاص بـ Réserve à l'établissement hospitalier بالمؤسسة الإستشفائية\n",
      "تاريخ ا الدخول Date d'entrée\n",
      "تاريخ ا عِنْدَ الخروج Date de sortie\n",
      "مَدار المصاريف Montant des frais\n",
      "المعرف الجبائي Matricule Fiscal\n",
      "Visa et cachet de l'établissement hospitalier\n",
      "إمضاء وختم المستشفى أو المصحة\n",
      "خاص بـ بتأمينات ستار réservé à la STAR\n",
      "ا مصاريف رمز العنوان\n",
      "إمضاء وَخاتم ا المراقب Visa & cachet du vérificateur\n",
      "‎bone‏ ل Frais engagés رَمْر العُنُول Code rubrique\n",
      "OBSERVATIONS\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "import pytesseract\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = r\"C:\\Users\\jlassi\\Desktop\\EyeQ_app\\data\\BS\\BsStar\\0741--9550517--20230705_page_0.jpg\"\n",
    "JSON_PATH = r\"C:\\Users\\jlassi\\Desktop\\testYolo\\notebooks\\mmm\\combined_layout.json\"\n",
    "OUTPUT_TEXT_PATH = \"document_text.txt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize EasyOCR for Arabic and English\n",
    "reader = easyocr.Reader(['ar', 'en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "# Configure Tesseract for handwritten text\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"  # Adjust path for Windows\n",
    "tesseract_config = \"--oem 1 --psm 6\"  # OEM 1: LSTM-based, PSM 6: Assume a single uniform block of text\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "if image is None:\n",
    "    raise FileNotFoundError(f\"Image not found at {IMAGE_PATH}\")\n",
    "image_height, image_width = image.shape[:2]\n",
    "\n",
    "# Load JSON layout\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    layout_data = json.load(f)\n",
    "\n",
    "lines = layout_data.get(\"lines\", [])\n",
    "tables = layout_data.get(\"tables\", {})\n",
    "\n",
    "# Function to crop image with bounds checking\n",
    "def safe_crop(image, bbox):\n",
    "    x_min, y_min, x_max, y_max = [int(x) for x in bbox]\n",
    "    x_min = max(0, x_min - 10)  # Add padding\n",
    "    y_min = max(0, y_min - 10)\n",
    "    x_max = min(image.shape[1], x_max + 10)\n",
    "    y_max = min(image.shape[0], y_max + 10)\n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        return None\n",
    "    return image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "# Function to apply Qwen2VL OCR to a cropped region\n",
    "def apply_qwen2vl_ocr(crop):\n",
    "    if crop is None:\n",
    "        return \"\"\n",
    "    crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "    temp_img_path = \"temp_crop.png\"\n",
    "    crop_pil.save(temp_img_path)\n",
    "    prompt = \"Extract the plain text from the provided image as if you were reading it naturally. Support Arabic and French text. Do not hallucinate.\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": f\"file://{temp_img_path}\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    qwen_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    os.remove(temp_img_path)\n",
    "    return qwen_text if qwen_text else None\n",
    "\n",
    "# Function to apply Tesseract OCR to a cropped region (handwritten text)\n",
    "def apply_tesseract_ocr(crop):\n",
    "    if crop is None:\n",
    "        return \"\"\n",
    "    crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "    return pytesseract.image_to_string(crop_pil, config=tesseract_config, lang='ara+fra')\n",
    "\n",
    "# Function to detect text boxes with EasyOCR and apply OCR with fallback\n",
    "def extract_text_boxes(image, bbox):\n",
    "    crop = safe_crop(image, bbox)\n",
    "    if crop is None:\n",
    "        return []\n",
    "    # Use EasyOCR to detect text boxes\n",
    "    easyocr_results = reader.readtext(crop, detail=1, paragraph=False)\n",
    "    texts = []\n",
    "    for (bbox_points, text, confidence) in easyocr_results:\n",
    "        # Convert EasyOCR bbox (list of points) to [x_min, y_min, x_max, y_max]\n",
    "        x_coords = [point[0] for point in bbox_points]\n",
    "        y_coords = [point[1] for point in bbox_points]\n",
    "        top_left = (min(x_coords), min(y_coords))\n",
    "        bottom_right = (max(x_coords), max(y_coords))\n",
    "        # Convert coordinates to integers for slicing\n",
    "        y_start = max(0, int(top_left[1] - 5))\n",
    "        y_end = int(bottom_right[1] + 5)\n",
    "        x_start = max(0, int(top_left[0] - 5))\n",
    "        x_end = int(bottom_right[0] + 5)\n",
    "        sub_crop = crop[y_start:y_end, x_start:x_end]\n",
    "        if sub_crop.size == 0:\n",
    "            continue\n",
    "        # Try Qwen2VL first\n",
    "        qwen_text = apply_qwen2vl_ocr(sub_crop)\n",
    "        if qwen_text and len(qwen_text.split()) > 1:  # Check if meaningful text\n",
    "            text = qwen_text\n",
    "        else:\n",
    "            # Fallback to Tesseract for handwritten text\n",
    "            text = apply_tesseract_ocr(sub_crop)\n",
    "            text = text.strip() if text.strip() else \"Handwritten text not recognized\"\n",
    "        if text and not (len(text.split()) > 50 and all(c.isdigit() for c in text.replace(\" \", \"\"))):  # Filter long number sequences\n",
    "            texts.append({\n",
    "                \"text\": text,\n",
    "                \"box\": [top_left[0] + bbox[0], top_left[1] + bbox[1], \n",
    "                        bottom_right[0] + bbox[0], bottom_right[1] + bbox[1]]\n",
    "            })\n",
    "    return texts\n",
    "\n",
    "# Identify table headers (line immediately above table)\n",
    "table_headers = {}\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    table_y_min = table_bbox[1]\n",
    "    header_line = None\n",
    "    min_y_diff = float(\"inf\")\n",
    "    for i, line in enumerate(lines):\n",
    "        line_y_max = line[\"bounding_box\"][3]\n",
    "        if line_y_max <= table_y_min and table_y_min - line_y_max < min_y_diff:\n",
    "            min_y_diff = table_y_min - line_y_max\n",
    "            header_line = i\n",
    "    if header_line is not None and min_y_diff < 50:\n",
    "        table_headers[table_key] = header_line\n",
    "\n",
    "# Group lines by left and right side\n",
    "midpoint = image_width / 2\n",
    "left_lines = []\n",
    "right_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    line_x_min = line[\"bounding_box\"][0]\n",
    "    if line_x_min < midpoint:\n",
    "        left_lines.append(i)\n",
    "    else:\n",
    "        right_lines.append(i)\n",
    "\n",
    "# Process lines\n",
    "line_texts = [[] for _ in lines]\n",
    "for i, line in enumerate(lines):\n",
    "    if i in table_headers.values():\n",
    "        continue  # Skip header lines, processed with tables\n",
    "    bbox = line[\"bounding_box\"]\n",
    "    texts = extract_text_boxes(image, bbox)\n",
    "    line_texts[i] = texts\n",
    "\n",
    "# Process tables\n",
    "table_texts = {key: [] for key in tables}\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    texts = extract_text_boxes(image, table_bbox)\n",
    "    table_texts[table_key] = texts\n",
    "    # Process header\n",
    "    header_idx = table_headers.get(table_key)\n",
    "    if header_idx is not None:\n",
    "        header_bbox = lines[header_idx][\"bounding_box\"]\n",
    "        header_texts = extract_text_boxes(image, header_bbox)\n",
    "        line_texts[header_idx] = header_texts\n",
    "\n",
    "# Format output\n",
    "formatted_text = []\n",
    "\n",
    "# Process left-side lines (French, left-to-right)\n",
    "for line_idx in sorted(left_lines, key=lambda i: lines[i][\"bounding_box\"][1]):\n",
    "    if line_idx in table_headers.values():\n",
    "        continue\n",
    "    texts = line_texts[line_idx]\n",
    "    if not texts:\n",
    "        continue\n",
    "    texts.sort(key=lambda t: t[\"box\"][0])  # Left-to-right\n",
    "    line_text = \" \".join(t[\"text\"] for t in texts if t[\"text\"].strip())\n",
    "    if line_text:\n",
    "        formatted_text.append(line_text)\n",
    "\n",
    "# Process right-side lines (Arabic, right-to-left)\n",
    "for line_idx in sorted(right_lines, key=lambda i: lines[i][\"bounding_box\"][1]):\n",
    "    if line_idx in table_headers.values():\n",
    "        continue\n",
    "    texts = line_texts[line_idx]\n",
    "    if not texts:\n",
    "        continue\n",
    "    texts.sort(key=lambda t: -t[\"box\"][0])  # Right-to-left for Arabic\n",
    "    line_text = \" \".join(t[\"text\"] for t in texts if t[\"text\"].strip())\n",
    "    if line_text:\n",
    "        formatted_text.append(line_text)\n",
    "\n",
    "# Process tables\n",
    "for table_key, table_coords in tables.items():\n",
    "    table_bbox = [float(x) for x in table_key.strip(\"()\").split(\",\")]\n",
    "    texts = table_texts[table_key]\n",
    "    if not texts:\n",
    "        continue\n",
    "    col_names = []\n",
    "    col_positions = []\n",
    "    header_idx = table_headers.get(table_key)\n",
    "    if header_idx is not None:\n",
    "        header_texts = line_texts[header_idx]\n",
    "        header_texts.sort(key=lambda t: t[\"box\"][0])\n",
    "        col_names = [t[\"text\"] for t in header_texts if t[\"text\"].strip()]\n",
    "        col_positions = [(t[\"box\"][0] + t[\"box\"][2]) / 2 for t in header_texts]\n",
    "    texts.sort(key=lambda t: t[\"box\"][1])\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    current_y = None\n",
    "    y_tolerance = 20\n",
    "    for text in texts:\n",
    "        y = text[\"box\"][1]\n",
    "        if current_y is None or abs(y - current_y) < y_tolerance:\n",
    "            current_row.append(text)\n",
    "            current_y = y if current_y is None else current_y\n",
    "        else:\n",
    "            if current_row:\n",
    "                rows.append(current_row)\n",
    "            current_row = [text]\n",
    "            current_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    aligned_rows = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda t: t[\"box\"][0])\n",
    "        row_positions = [(t[\"box\"][0] + t[\"box\"][2]) / 2 for t in row]\n",
    "        aligned_row = [\"\"] * len(col_names)\n",
    "        for text, pos in zip([t[\"text\"] for t in row], row_positions):\n",
    "            if col_positions:\n",
    "                closest_col_idx = min(range(len(col_positions)), key=lambda i: abs(col_positions[i] - pos))\n",
    "                aligned_row[closest_col_idx] = text\n",
    "        aligned_rows.append(aligned_row)\n",
    "    max_widths = [max(len(col_names[i]) if i < len(col_names) else 0,\n",
    "                      max((len(row[i]) if i < len(row) else 0) for row in aligned_rows))\n",
    "                  for i in range(max(len(col_names), max(len(row) for row in aligned_rows)))]\n",
    "    if col_names:\n",
    "        formatted_text.append(f\"\\nTable {table_key}:\")\n",
    "        header_row = \" | \".join(col_names[i].ljust(max_widths[i]) if i < len(col_names) else \"\".ljust(max_widths[i])\n",
    "                               for i in range(len(max_widths)))\n",
    "        formatted_text.append(header_row)\n",
    "        formatted_text.append(\"-\" * len(header_row))\n",
    "    for row in aligned_rows:\n",
    "        row_text = \" \".join(row[i].ljust(max_widths[i]) if i < len(row) else \"\".ljust(max_widths[i])\n",
    "                           for i in range(len(max_widths)))\n",
    "        formatted_text.append(row_text)\n",
    "\n",
    "# Save and print formatted text\n",
    "with open(OUTPUT_TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(formatted_text))\n",
    "\n",
    "print(\"Formatted Text:\")\n",
    "print(\"\\n\".join(formatted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formalized Text:\n",
      "**Patient Information**\n",
      "\n",
      "* **Name:** [Insert name]\n",
      "* **Date of Birth:** [Insert date of birth]\n",
      "* **Address:** [Insert address]\n",
      "\n",
      "**Medical History**\n",
      "\n",
      "* **Chief Complaint:** [Insert chief complaint]\n",
      "* **Past Medical History:** [Insert past medical history]\n",
      "* **Medications:** [Insert medications]\n",
      "\n",
      "**Admission Information**\n",
      "\n",
      "* **Date of Admission:** [Insert date of admission]\n",
      "* **Time of Admission:** [Insert time of admission]\n",
      "* **Reason for Admission:** [Insert reason for admission]\n",
      "\n",
      "**Treatment and Services**\n",
      "\n",
      "| Service | Date | Time | Provider |\n",
      "| --- | --- | --- | --- |\n",
      "| [Insert service] | [Insert date] | [Insert time] | [Insert provider] |\n",
      "\n",
      "**Charges**\n",
      "\n",
      "| Category | Amount |\n",
      "| --- | --- |\n",
      "| [Insert category] | [Insert amount] |\n",
      "| [Insert category] | [Insert amount] |\n",
      "\n",
      "**Insurance Information**\n",
      "\n",
      "* **Insurance Company:** [Insert insurance company]\n",
      "* **Policy Number:** [Insert policy number]\n",
      "* **Effective Date:** [Insert effective date]\n",
      "\n",
      "**Observations**\n",
      "\n",
      "* [Insert observations]\n",
      "\n",
      "Note: The above output is a formalized version of the input text, with sections and tables properly formatted. All valid content has been preserved, except for the specified hallucinations (i.e., the repeated \"بمَنْحِهِمْ\" phrases).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "EXTRACTED_TEXT_PATH = \"document_text.txt\"\n",
    "FORMALIZED_TEXT_PATH = \"formalized_text.txt\"\n",
    "\n",
    "# Read the extracted text\n",
    "with open(EXTRACTED_TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted_text = f.read()\n",
    "\n",
    "# Define the prompt for llama3 via Ollama\n",
    "prompt = f\"\"\"\n",
    "### Context:\n",
    "You are an advanced language model tasked with cleaning up and formalizing text extracted from a medical care form (Bulletin de soins). The form contains information in both Arabic and French, related to patient details, hospital information, and a table of medical acts. The extracted text contains specific errors such as:\n",
    "- Long sequences of numbers (e.g., '86, 87, 88, ...') that are hallucinations.\n",
    "- Japanese text (e.g., '## 2020.04.18 ## ## 11 月 1 1 日') that doesn't belong.\n",
    "- Misaligned or poorly formatted lines and tables.\n",
    "\n",
    "### Task:\n",
    "Your task is to:\n",
    "1. Clean up the extracted text by removing only the specific hallucinations (long number sequences and Japanese text) while preserving all other valid content.\n",
    "2. Correct language inconsistencies (ensure only Arabic and French text remains, with proper context).\n",
    "3. Organize the text into a formal structure, respecting the form’s sections (e.g., patient info, hospital info, tables).\n",
    "4. Ensure proper formatting for tables, with aligned columns and headers.\n",
    "5. Preserve the original meaning and content as much as possible, avoiding deletion of valid text.\n",
    "\n",
    "### Types of Corrections:\n",
    "- **Remove Hallucinations**: Remove long sequences of numbers (e.g., '86, 87, 88, ...') and Japanese text (e.g., '## 2020.04.18 ## ## 11 月 1 1 日'), but keep all other text.\n",
    "- **Language Consistency**: Ensure Arabic text is coherent and French text is coherent. Remove only non-Arabic/French text (e.g., Japanese).\n",
    "- **Formal Structure**: Organize the text into clear sections (e.g., 'Patient Information', 'Hospital Information', 'Medical Acts Table').\n",
    "- **Table Formatting**: Ensure the table has proper headers and aligned rows.\n",
    "- **Text Correction**: Fix obvious OCR errors (e.g., 'Adhérent 00 Conjoint' should be separated into fields).\n",
    "\n",
    "### Extracted Text:\n",
    "{extracted_text}\n",
    "\n",
    "### Output:\n",
    "Provide the formalized text in a clean, structured format with sections and tables properly formatted, preserving all valid content except the specified hallucinations.\n",
    "\"\"\"\n",
    "\n",
    "# Use Ollama to process the extracted text with llama3\n",
    "response = ollama.generate(\n",
    "    model=\"llama3\",\n",
    "    prompt=prompt,\n",
    "    options={\n",
    "        \"temperature\": 0.0,  # Disable randomness for deterministic output\n",
    "        \"num_predict\": 1500,  # Max tokens to generate\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the formalized text from the response\n",
    "formalized_text = response[\"response\"].split(\"### Output:\")[-1].strip() if \"### Output:\" in response[\"response\"] else response[\"response\"].strip()\n",
    "\n",
    "# Save the formalized text\n",
    "with open(FORMALIZED_TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(formalized_text)\n",
    "\n",
    "print(\"Formalized Text:\")\n",
    "print(formalized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
